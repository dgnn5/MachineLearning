# MachineLearning
Goal: To make a functional Neural Network in Python from scratch (minimal imports and no frameworks).
Note: "Artificial Intelligence.py" is the skeleton of the Neural Network, whereas "SamanthaAI.py" is the fully implemented AI with training, etc.

Preface:
Neural Networks are a model of computing to allow computers to take input data and learn what the output data should be (ex: classifying shapes, predicting responses, interpreting large amounts of data). A Neural Network (at least this model of Neural Network (I believe this model is called a "dense fully connected" N.N.)) is made of 2 primary components: Nodes and Connections. The nodes are split into different layers; the Input-Layer, Hidden-Layers, and Output-Layer. Each node has an associated floating point "value" (typically bounded between 0 and 1). Each node is connected to every node in the previous layer. These connections have a floating point Weight and a floating point Bias to them (typically bounded between -1 and 1). The values of the nodes in the Input-Layer are set to values dependant on the training data. The value of every other node can be calculated from these input nodes and their weights and biases. By "Propogating" through the layers to the output layer, the value of the output layer nodes can be calculated, which are then interpreted dependent on the training data to figure out what the Neural Network predicts the output to be. This prediction is graded and averaged with all of it's other predictions from the data to determine the accuracy of this network. This accuracy is compared to many other networks to determine the best NN's, which are then chosen to generate new NN's (This process is called "evolution" training). This is repeated until the desired accuracy is achieved.

There is another method of training known as Back Propogation, in which a single NN is trained. Real Back Propogation is beyond my understanding, but I approximated the training by choosing a mutation modifier (some constant float ~0.01) and either doing nothing, adding the mutation modifier, or subtracting the mutation modifier to each weight and bias of the chosen NN and comparing the new mutated NN to its original version. If the new version is more accurate, then repeat this process with that NN. Repeat until the desired accuracy is achieved.

The training data for this project is in the form of 5 input integers (0-9 inclusive), and an output boolean. The value of the output was determined based on a variety of if's and elif's (such as "if the 1st number == the 2nd number and the 5th number times the 4th number == the third number", etc). Unfortunately the exact ifs and elifs have been lost, but the training data was saved. The goal was to get the NN to accurately predict whether any given input integers should output true or false.

Process:
First, I had to learn what an NN was and how it works, so I did a decent bit of research and learned a lot of the abstract concepts from a friend (who I was "competing" against with similar training data and restrictions on imports to see who could get the highest accuracy).
After learning the math and abstractions, I began work on the node classes. I separated the nodes into 3 different categories: Layer0 nodes (input-layer), Layer1 nodes, and HiddenLayer nodes. The Layer0 nodes are the input nodes, so they have no weights associated with them; They only have values. The Layer1 nodes are technically a part of the hidden-layers, though I separated them because the Layer1 nodes will have a different number of connections than the rest of the hidden-layer nodes. Each Layer1 node will have a number of connections equal to the number of Layer0 nodes. The HiddenLayer nodes make up the rest of the nodes in the NN (including the output node). They each have a number of connections equal to the number of nodes per hidden-layer. Both the Layer1 and HiddenLayer type nodes have a Propogate() member function, which required some function to bound the values of the nodes between 0 and 1. I chose the Sigmoid function for this, since that is the most common and easiest to implement in simple NN's.

Next, I worked on creating the skeleton of the NN. The create_skeleton() function was used for testing, so that I could directly input values for all of the NN, and see it printed in a neat and easily readable format. The auto_create_skeleton() function was an improved version of the create_skeleton() function, since the user didn't have to type anything into the terminal at runtime. Instead they could simply input all the necessary information as parameters in the function.

Then, I created the create_weights() function, which simply generated weights for each of the nodes in the hidden-layers, and the propogate() function, which called the Propogate() member function for each of the nodes in the hidden-layers. After all of those functions were created, I made the main body for "Artificial Intelligence.py", which was used to test all of the functions. At this point, I copied the code over to a new file, "SamanthaAI.py" and began work on expanding the code and specializing it for the training data.

The first thing I worked on is the auto_create_inLayer() function, which creates an array of Layer0-type nodes, called inLayer, and sets their values based on the input parameter array. Next, I created a Parent class to contain entire NN's inside one class (also setting the foundation for evolution training). This class contains 2 lists of nodes: hiddenLayers, which is an array of each hidden layer (each hidden layer is stored as either an array of Layer1-type nodes (for the first hidden-layer), or HiddenLayer-type nodes (for the rest of the hidden-layers)), and outLayer, which is an array of HiddenLayer-type nodes.

At this point I realized that I would have to find some way to recreate the NN's once I generated one with a high accuracy, so I added the str(self) member function to the Parent class, which creates a list of all of the important values of the NN and returns a string of that list. This way, I could create a function later to interpret the list as a parameter and create a Parent class from it.

Next, I created a few functions to test the NN's: The get_answer() function, which interprets the output node (This project only has one node in the outLayer) to obtain the NN's predicted answer (True if the value of the output node is greater than or equal to 0.5, and False otherwise); The get_score() function, which returns a percent score depending on the total correct answers and the total questions; And the take_test() function, which tests a single Parent NN on data within the training data file and returns the percent score. Then I made the create_generation() function, which creates an array of Parent NN's. After that, I worked on some functons to test multiple NN's: test_all() tests every NN in a generation and returns an array of tuples, each containing the NN and that NN's score; find_winners() finds the top few scores and returns a smaller version of the list of tuples from the previous function; best_so_far(), which kept track of the top few scores in each generation (omitted due to it not being used); best_in_generation(), which finds and prints the highest score, and the NN that it belongs to; generation_0_test(), which creates a generation of Parents and tests all of them, returning the top few.

After this, I began work on the actual training. For this, I decided to go with Evolution model, since it is the simplest type of training I could understand. To do this, I broke the training into a few different functions to follow the process better. create_child() (Later replaced with better_create_child()) takes a number of input Parent NN's and averages all of their weights and biases, applying mutations to each before reconstructing these weights and biases into a new child NN; better_create_child() does the same thing, but with less input parameters required; evolution_step() creates and returns the next generation from a given set of Parent NN's, the generation size, and the number of parents per child; evolution_big_step() takes care of the testing and grading of each NN within a generation, as well as creating a number of generations set by the function parameters; Evolution() takes a lot of input parameters to create generation 0 and applies evolution training to the NN's.

At this point, I was mostly finished with Evolution training, and the only thing left was to test the NN's after they have gone through training. To do this, I had to create a few functions to be able to interpret the list of weights and biases which were output during the training: create_brain_from_weights() took input lists and returned a brain (NN) created with the given values; auto_create_brain_from_weights() was an improved version of create_brain_from_weights(), as more of the input parameters were compressed into lists; full_auto_brain() is the most improved version of these functions, as it only requires a copy/paste of the output list (the array output during training) in order to generate and return an NN constructed with the given weights and biases.

Finally, I began training, though the highest accuracy I could achieve was somewhere around 80% accurate. I wanted to reach 90% or higher, so I added an additional method of training known as Back Propogation. In order to do this, I created a few new functions: back_propogation_step() creates a copy of the input NN and loops through every weight and bias and either leaves it alone, adds the mutation factor, or subtracts the mutation factor, then it tests both the copy and the original brain and returns whichever NN has a higher score; back_propogation() calls back_propogation_step() many times; BackPropogation() does this for a whole generation of NN's and returns the best out of all of them.
