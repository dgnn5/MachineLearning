# MachineLearning
Goal: To make a functional Neural Network in Python from scratch (minimal imports and no frameworks).
Note: "Artificial Intelligence.py" is the skeleton of the Neural Network, whereas "SamanthaAI.py" is the fully implemented AI with training, etc.

Preface:
Neural Networks are a model of computing to allow computers to take input data and learn what the output data should be (ex: classifying shapes, predicting responses, interpreting large amounts of data). A Neural Network (at least this model of Neural Network (I believe this model is called a "dense fully connected" N.N.)) is made of 2 primary components: Nodes and Connections. The nodes are split into different layers; the Input-Layer, Hidden-Layers, and Output-Layer. Each node has an associated floating point value (typically bounded between 0 and 1). Each node is connected to every node in the previous layer. These connections have a floating point Weight and a floating point Bias to them (typically bounded between -1 and 1). The values of the nodes in the Input-Layer are set to values dependant on the training data. The value of every other node can be calculated from these input nodes and the weights and biases. By "Propogating" down the layers to the output layer, the value of the output layer nodes can be calculated, which are then interpreted dependent on the training data to figure out what the Neural Network predicts the output to be. This prediction is graded and averaged with all of it's other predictions from the data to determine the accuracy of this network. This accuracy is compared to many other networks to determine the best NN's, which are chosen to generate new NN's. This is repeated until the desired accuracy is achieved.

The training data for this project is in the form of 5 input integers (0-9 inclusive), and an output boolean. The value of the output was determined based on a variety of if's and elif's (such as "if the 1st number == the 2nd number and the 5th number times the 4th number == the third number", etc). Unfortunately the exact ifs and elifs have been lost to time, but the training data was saved. The goal was to get the NN to accurately predict whether any given input integers should output true or false.

Process:
First, I had to learn what an NN was and how it works, so I did a decent bit of research and learned a lot of the abstract concepts from a friend (who I was "competing" against with similar training data and restrictions on imports to see who could get the highest accuracy).
After learning the math and abstractions, I began work on the node classes. I separated the nodes into 3 different categories: Layer0 nodes (input-layer), Layer1 nodes, and HiddenLayer nodes. The Layer0 nodes are the input nodes, so they have no weights associated with them; They only have values. The Layer1 nodes are technically a part of the hidden-layers, though I separated them because the Layer1 nodes will have a different number of weights than the rest of the hidden-layer nodes. Each Layer1 node will have a number of weights equal to the number of Layer0 nodes. The HiddenLayer nodes make up the rest of the nodes in the NN (including the output node). They each have a number of weights equal to the number of nodes per hidden-layer. Both the Layer1 and HiddenLayer type nodes have a Propogate() member function, which required some function to bound the values of the nodes between 0 and 1. I chose the Sigmoid function for this, since that is the most common and easiest to implement in simple NN's.

Next, I worked on creating the skeleton of the NN. The create_skeleton() function was used for testing, so that I could directly input values for all of the NN, and see it printed in a neat and easily readable format. The auto_create_skeleton() function was an improved version of the create_skeleton() function, since the user didn't have to type anything into the terminal at runtime. Instead they could simply input all the necessary information as parameters in the function.

Then, I created the create_weights() function, which simply generated weights for each of the nodes in the hidden-layers, and the propogate() function, which called the Propogate() member function for each of the nodes in the hidden-layers. After all of those functions were created, I made the main body for "Artificial Intelligence.py", which was used to test all of the functions. At this point, I copied the code over to a new file, "SamanthaAI.py" and began work on expanding the code and specializing it for the training data.

The first thing I worked on is the auto_create_inLayer() function, which creates an array of Layer0-type nodes, called inLayer, and sets their values based on the input parameter array. Next, I created a Parent class to contain all of the NN inside one class (also setting the foundation for evolution training). This class contains 2 lists of nodes: hiddenLayers, which is an array of each hidden layer (each hidden layer is stored as an array of Layer1-type nodes (for the first hidden-layer), or HiddenLayer-type nodes (for the rest of the hidden-layers)), and outLayer, which is an array of HiddenLayer-type nodes.
